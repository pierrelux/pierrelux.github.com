<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Pierre-Luc Bacon</title>
    <link href="https://fonts.googleapis.com/css?family=Libre+Baskerville|Roboto&display=swap" rel="stylesheet">
    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css" rel="stylesheet">
    <link href="/css/custom.css" rel="stylesheet">

      </head>

  <body>
    <nav class="navbar navbar-default navbar-custom">
        <div class="container-fluid">
            <div class="navbar-header">
                <a class="page-scroll navbar-brand" href="/">Pierre-Luc Bacon</a>
            </div>

            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a class="page-scroll" href="/teaching">Teaching</a>
                </li>
                <li>
                    <a class="page-scroll" href="/contact">Contact</a>
                </li>
            </ul>
        </div>
    </nav>
    <div class="container container-article">
    <div class="frontpage">
    <h1 id="excursions-in-reinforcement-learning">Excursions in Reinforcement Learning</h1>
    <p>This course (<a href="https://admission.umontreal.ca/cours-et-horaires/cours/ift-6760c/">IFT6760C</a>) is intended for advanced graduate students with a good background in machine learning, mathematics, operations research or statistics. You can register to <a href="https://admission.umontreal.ca/cours-et-horaires/cours/ift-6760c/">IFT6760C</a> on <a href="https:///academique-dmz.synchro.umontreal.ca">Synchro</a> if your affiliation is with UdeM, or via the <a href="https://mobilite-cours.crepuq.qc.ca/4DSTATIC/ENAccueil.html">CREPUQ</a> if you are from another institution. Due to the research-oriented nature of this class, you need to be comfortable with a teaching format involving <em>open-ended</em> questions and assignments. You will be required to think critically and adopt an open mindset. My teaching goal with this course is for all the participants to build their own understanding of reinforcement learning in relation to their primary research area while sharing their unique perspective and insights with the entire class.</p>
    <p><a href="https://www.lexico.com/en/definition/excursion"><em>Excursion</em></a>:</p>
    <ul>
    <li><em>A short journey or trip, especially one taken as a leisure activity</em>.</li>
    <li><em>A deviation from a regular activity or course.</em></li>
    </ul>
    <p>Origin: from the Latin verb <em>excurrere</em> which means <em>to run out</em>. This is also the intended meaning behind the title of this course. I want us to deviate from the usual paths and explore the rich connections between reinforcement learning and other disciplines, in particular: optimization, control theory and simulation. And of course, I'm also hoping that this will be a fun activity for everyone.</p>
    <h2 id="time-and-location">Time and Location</h2>
    <p>Twice a week, on Tuesday from 9:30 to 11:30AM and on Friday from 13h30 to 15h40. The course is taught at <a href="https://mila.quebec/">Mila</a> in the Agora of the <a href="https://goo.gl/maps/8xD9WVFZKdNr633CA">6650 Saint-Urbain</a>. You don't need badge access to enter the classroom. Here's a <a href="https://photos.app.goo.gl/StJzkjokQqQ9eqSw5">video</a> showing you how to access the classroom from Saint-Zotique Ouest.</p>
    <h2 id="evaluation">Evaluation</h2>
    <p>The following evaluation structure is subject to change depending on the class size.</p>
    <ul>
    <li>Class participation: 5%</li>
    <li>Assignments: 3 x 15% = 45%</li>
    <li>Final project: 50%</li>
    </ul>
    <h2 id="recommended-textbooks">Recommended Textbooks</h2>
    <p>There is no mandatory textbook. I will however be referencing content from:</p>
    <ul>
    <li><a href="https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887">Markov Decision Processes: Discrete Stochastic Dynamic Programming</a> by Martin Puterman</li>
    <li><a href="https://web.mit.edu/dimitrib/www/RLbook.html">Reinforcement Learning and Optimal Control</a> by Dimitri Bertsekas</li>
    <li><a href="http://incompleteideas.net/book/the-book.html">Reinforcement Learning: An Introduction</a> by Richard S. Sutton and Andrew G. Barto</li>
    <li><a href="http://www.athenasc.com/nonlinbook.html">Nonlinear Programming: 3rd Edition</a> by Dimitri Bertsekas</li>
    <li><a href="https://epubs.siam.org/doi/book/10.1137/1.9780898718577">Practical Methods for Optimal Control and Estimation Using Nonlinear Programming</a> by John T. Betts</li>
    <li><a href="https://epubs.siam.org/doi/book/10.1137/1.9780898717761">Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation, Second Edition</a> by Andreas Griewank and Andrea Walther</li>
    </ul>
    <h2 id="schedule">Schedule</h2>
    <h3 id="january-7">January 7</h3>
    <p>First class. Markov Decision Processes, induced process</p>
    <h3 id="january-10">January 10</h3>
    <p>Examples of MDPs, constrained MLE as sequential allocation, criteria: finite horizon, infinite horizon, average reward, random horizon interpretation of infinite discounted setting. Bellman optimality</p>
    <h3 id="january-14">January 14</h3>
    <ul>
    <li>Absorbing states and discounting.
    <ul>
    <li>Section 5.3, proposition 5.3.1 and page 127 in Puterman (1994)</li>
    <li><a href="https://ieeexplore.ieee.org/document/917668">Death and discounting</a> by Adam Shwartz</li>
    <li><pre><code>@article{Shwartz2001,
    doi = {10.1109/9.917668},
    url = {https://doi.org/10.1109/9.917668},
    year = {2001},
    month = apr,
    publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
    volume = {46},
    number = {4},
    pages = {644--647},
    author = {A. Shwartz},
    title = {Death and discounting},
    journal = {{IEEE} Transactions on Automatic Control}
    }</code></pre></li>
    </ul></li>
    <li><p>Vector notation: Section 5.6 in Puterman (1994)</p></li>
    <li><p>Neumann series and policy evaluation: Theorem 6.1.1 and corrolary C.4 in Puterman (1994)</p></li>
    <li><p>Matrix splitting methods:</p>
    <ul>
    <li><a href="http://www.sam.math.ethz.ch/~mhg/unt/SWNLA/itmethSWNLA08.pdf">Iterative Methods</a> by Martin H. Gutknecht</li>
    <li><a href="https://atrium.umontreal.ca/primo-explore/search?query=any,contains,Matrix%20iterative%20analysis&amp;tab=default_tab&amp;search_scope=Tout_sauf_articles&amp;sortby=date&amp;vid=UM&amp;facet=frbrgroupid,include,340907508&amp;lang=fr_FR&amp;offset=0">Matrix iterative analysis</a> by Richard Varga</li>
    <li><pre><code>@book{varga1962,
    title       = {Matrix iterative analysis},
    author  = {Richard S. Varga},
    lccn        = {62021277},
    year        = {1962},
    publisher   = {Prentice-Hall},
    address = {Englewood Cliffs}
    }</code></pre></li>
    </ul></li>
    </ul>
    <h3 id="week-of-january-17">Week of January 17</h3>
    <ul>
    <li><p>Primal and Dual LP formulations: section 6.9.1 in Puterman (1994)</p>
    <ul>
    <li><a href="https://atrium.umontreal.ca/primo-explore/fulldisplay?docid=UM-ALEPH001676491&amp;context=L&amp;vid=UM&amp;search_scope=Tout_sauf_articles&amp;tab=default_tab&amp;lang=fr_FR">Finite State Markovian Decision Processes</a> by Cyrus Derman</li>
    <li><pre><code>@book{Derman1970,
     author = {Derman, Cyrus},
     title = {Finite State Markovian Decision Processes},
     year = {1970},
     isbn = {0122092503},
     publisher = {Academic Press, Inc.},
     address = {Orlando, FL, USA},
    } </code></pre></li>
    <li><a href="https://www.math.leidenuniv.nl/~kallenberg/Lecture-notes-MDP.pdf">Markov Decision Processes</a> by Lodewijk Kallenberg</li>
    <li><a href="https://www-sop.inria.fr/members/Eitan.Altman/TEMP/h.pdf">Constrained Markov Decision Processes</a> by Eitan Altman</li>
    <li><pre><code>@book{altman1999,
      author    = {Altman, E.},
      publisher = {Chapman and Hall},
      title     = {Constrained Markov Decision Processes},
      year      = 1999,
      pages     = 256
    }</code></pre></li>
    </ul></li>
    <li><p>Occupation measures, induced randomized stationary policy: theorem 6.9.1 in Puterman (1994)</p></li>
    <li><p>Characterization of basic feasible solutions as Markov Deterministic decision rules: proposition 6.9.3 in Puterman (1994)</p></li>
    <li><p>Bias of the MRP in the average reward formulation: section 8.2.1 in Puterman (1994)</p></li>
    <li><p>Laurent series expansion in the average reward forumulation: section 8.2.2 in Puterman (1994)</p></li>
    <li><p>Representation of the total expected discounted reward in terms of the interest rate</p></li>
    <li><p>Corollary 8.2.4: decomposition of the total expected discounted reward of a policy into the gain and bias terms</p></li>
    <li><p>Corollary 8.2.5: gain as the limit of discount factor going to 1 in the total expected discounted reward</p></li>
    <li><p>Bellman optimality equations in the total expected discounted reward criterion</p></li>
    <li><p>Equality between taking the maximum over the set of deterministic Markovian decision rules instead of the randomized Markovian decision rules: proposition 6.2.1 in Puterman (1994)</p></li>
    <li><p>Contraction mappings: section 6.2.3 in Puterman (1994)</p></li>
    <li><p>Banach fixed-point theorem: theorem 6.2.3 in Puterman (1994)</p></li>
    </ul>
    <h3 id="week-of-january-27">Week of January 27</h3>
    <p>LSTD(lambda), TD(lambda), oblique perspective, variational inequality perspective, stability</p>
    <h3 id="week-of-february-3">Week of February 3</h3>
    <p>Off-policy learning: importance sampling and the conditional monte-carlo method</p>
    <h3 id="week-of-february-10">Week of February 10</h3>
    <p>Fitted value methods: FQI, NFQI, DQN, proximal methods and GTD/TDC</p>
    <h3 id="week-of-february-17">Week of February 17</h3>
    <p>Policy gradients: occupation measures, discounted objective, implicit differentiation and derivation in the infinite horizon case</p>
    <h3 id="week-of-february-24">Week of February 24</h3>
    <p>Policy gradients: derivative estimation, likelihood ratio methods (REINFORCE), reparametrization (IPA), baselines (control variates), actor-critic systems</p>
    <h3 id="week-of-march-2">Week of March 2</h3>
    <p>Spring break</p>
    <h3 id="week-of-march-9">Week of March 9</h3>
    <p>Policy gradients: application for learning temporal abstractions, the option-critic architecture, hierarchical and goal-conditioned RL</p>
    <h3 id="week-of-march-16">Week of March 16</h3>
    <p>Policy gradients: Linear-Quadratic Regulator, Lagrangian formulation, MPC, Monte-Carlo Tree Search</p>
    <h3 id="week-of-march-23">Week of March 23</h3>
    <p>Automatic differentiation as discrete-time optimal control</p>
    <h3 id="week-of-march-30">Week of March 30</h3>
    <p>Formulation of inverse RL and meta-RL as bilevel optimization.</p>
    <h3 id="week-of-april-6">Week of April 6</h3>
    <p>Methods (contd.): KKT &quot;trick&quot;, forward, reverse, implicit, competitive. Case studies</p>
    <h3 id="week-of-april-13">Week of April 13</h3>
    <p>Challenge and opportunities</p>
    <h3 id="week-of-april-20">Week of April 20</h3>
    <p>Final project presentations</p>
    </div>
    <h2 id="wellbeing">Wellbeing</h2>
    <p>Academic life can sometimes be overwhelming. Don't hesitate to find support:</p>
    <ul>
    <li>UdeM: <a href="http://www.cscp.umontreal.ca">Centre de santé et de consultation psychologique</a></li>
    <li>McGill: <a href="https://www.mcgill.ca/wellness-hub/">Student wellness hub</a></li>
    <li>Polytechnique: <a href="https://www.polymtl.ca/soutien/">Soutien à la réussite</a></li>
    <li>HEC: <a href="https://www.hec.ca/en/students/support-resources/psychological-support/index.html">Psychological support and ressources</a></li>
    </ul>
    </div>
    </div>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
     (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
       m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
         })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-68825837-1', 'auto');
    ga('send', 'pageview');

    </script>
  </body>
</html>
