<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Pierre-Luc Bacon</title>
    <link href="https://fonts.googleapis.com/css?family=Libre+Baskerville|Roboto&display=swap" rel="stylesheet">
    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css" rel="stylesheet">
    <link href="/css/custom.css" rel="stylesheet">

      </head>

  <body>
    <nav class="navbar navbar-default navbar-custom">
        <div class="container-fluid">
            <div class="navbar-header">
                <a class="page-scroll navbar-brand" href="/">Pierre-Luc Bacon</a>
            </div>

            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a class="page-scroll" href="/teaching">Teaching</a>
                </li>
                <li>
                    <a class="page-scroll" href="/group">Group</a>
                </li>
                <li>
                    <a class="page-scroll" href="/contact">Contact</a>
                </li>
            </ul>
        </div>
    </nav>
    <div class="container container-article">
    <div class="frontpage">
    <h1 id="reinforcement-learning-and-optimal-control">Reinforcement Learning and Optimal Control</h1>
    <p>This course (<a href="https://admission.umontreal.ca/cours-et-horaires/cours/ift-6760c/">IFT6760C</a>) is intended for advanced graduate students with a good background in machine learning,
       mathematics, operations research or statistics. You can register to <a href="https://admission.umontreal.ca/cours-et-horaires/cours/ift-6760c/">IFT6760C</a> 
       on <a href="https:///academique-dmz.synchro.umontreal.ca">Synchro</a> if your affiliation is with UdeM, or via the <a href="https://mobilite-cours.crepuq.qc.ca/4DSTATIC/ENAccueil.html">CREPUQ</a> 
       if you are from another institution. Due to the research-oriented nature of this class, you need to be comfortable with a teaching format involving <em>open-ended</em> questions and assignments. 
       You will be required to think critically and adopt an open mindset. My teaching goal with this course is for all the participants to build their own understanding of reinforcement learning in 
       relation to their primary research area while sharing their unique perspective and insights with the entire class.</p>

    <h2 id="topics">Sujets/Topics</h2>
    <li> Processus de décision markovien, formulation sous forme de programme linéaire, forme lisse des équations de Bellman, équations de Bellman projettées, analyse des algorithmes de type TD, 
      estimation de dérivées, commande optimale en temps continu et discret, principe du maximum de Pontryagin, Hamiltonien en temps discret et en temps continu, méthode par état adjoint et 
      méthode variationelle pour le calcul de sensibilité, méthodes de contrôle directes et indirectes, apprentissage par renforcement inversé, et plus!</li>

    <li>Markov Decision Processes, LP formulation, occupation measure, smooth bellman equations, projected bellman equations, analysis of TD algorithms, derivative estimation, 
    discrete and continuous optimal control, Pontryagin maximum principle, discrete and continuous time Hamiltonian,
    adjoint and forward sensitivity equations, single shooting, multiple shooting, collocation methods, inverse reinforcement learning, and more!</li>

    <h2 id="evaluation">Evaluation</h2>
    <ul>
      <li>New RL problem (15%): pick a control problem from the literature, describe it, wrap it (python), solve it. </li>
      <li>Final project (25%)</li>
      <li>Assignments: (4 x 15% = 60%): every two weeks</li>
    </ul>
    <h2 id="recommended-textbooks">Recommended Textbooks</h2>
    <p>There is no mandatory textbook. I will however be referencing content from:</p>
    <ul>
    <li><a href="https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887">Markov Decision Processes: Discrete Stochastic Dynamic Programming</a> by Martin Puterman</li>
    <li><a href="https://web.mit.edu/dimitrib/www/RLbook.html">Reinforcement Learning and Optimal Control</a> by Dimitri Bertsekas</li>
    <li><a href="http://incompleteideas.net/book/the-book.html">Reinforcement Learning: An Introduction</a> by Richard S. Sutton and Andrew G. Barto</li>
    <li><a href="https://epubs.siam.org/doi/book/10.1137/1.9780898718577">Practical Methods for Optimal Control and Estimation Using Nonlinear Programming</a> by John T. Betts</li>
    <li><a href="https://onlinelibrary.wiley.com/doi/book/10.1002/9780470230381">Simulation and the Monte Carlo Method, Second Edition</a> by Rubinstein and Kroese (2008)</li>
    </ul>

    <h2 id="wellbeing">Wellbeing</h2>
    <ul>
    <li>UdeM: <a href="http://www.cscp.umontreal.ca">Centre de santé et de consultation psychologique</a></li>
    <li>McGill: <a href="https://www.mcgill.ca/wellness-hub/">Student wellness hub</a></li>
    <li>Polytechnique: <a href="https://www.polymtl.ca/soutien/">Soutien à la réussite</a></li>
    <li>HEC: <a href="https://www.hec.ca/en/students/support-resources/psychological-support/index.html">Psychological support and ressources</a></li>
    <li>ASEQ: <a href="http://www.aseq.ca/rte/fr/FA%C3%89CUM_Programmedaide_Programmedaide">Student Health Support Program - Mental Health Resources</a></li>
    </ul>

    </div>
    </div>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
     (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
       m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
         })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-68825837-1', 'auto');
    ga('send', 'pageview');
    </script>
  </body>
</html>
