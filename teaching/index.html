<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Pierre-Luc Bacon</title>
    <link href="https://fonts.googleapis.com/css?family=Libre+Baskerville|Roboto&display=swap" rel="stylesheet">
    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css" rel="stylesheet">
    <link href="/css/custom.css" rel="stylesheet">

        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
      </head>

  <body>
    <nav class="navbar navbar-default navbar-custom">
        <div class="container-fluid">
            <div class="navbar-header">
                <a class="page-scroll navbar-brand" href="/">Pierre-Luc Bacon</a>
            </div>

            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a class="page-scroll" href="/teaching">Teaching</a>
                </li>
                <li>
                    <a class="page-scroll" href="/group">Group</a>
                </li>
                <li>
                    <a class="page-scroll" href="/contact">Contact</a>
                </li>
            </ul>
        </div>
    </nav>
    <div class="container container-article">
    <div class="frontpage">
    <h1 id="excursions-in-reinforcement-learning">Excursions in Reinforcement Learning</h1>
    <p>This course (<a href="https://admission.umontreal.ca/cours-et-horaires/cours/ift-6760c/">IFT6760C</a>) is intended for advanced graduate students with a good background in machine learning, mathematics, operations research or statistics. You can register to <a href="https://admission.umontreal.ca/cours-et-horaires/cours/ift-6760c/">IFT6760C</a> on <a href="https:///academique-dmz.synchro.umontreal.ca">Synchro</a> if your affiliation is with UdeM, or via the <a href="https://mobilite-cours.crepuq.qc.ca/4DSTATIC/ENAccueil.html">CREPUQ</a> if you are from another institution. Due to the research-oriented nature of this class, you need to be comfortable with a teaching format involving <em>open-ended</em> questions and assignments. You will be required to think critically and adopt an open mindset. My teaching goal with this course is for all the participants to build their own understanding of reinforcement learning in relation to their primary research area while sharing their unique perspective and insights with the entire class.</p>
    <p><a href="https://www.lexico.com/en/definition/excursion"><em>Excursion</em></a>:</p>
    <ul>
    <li><em>A short journey or trip, especially one taken as a leisure activity</em>.</li>
    <li><em>A deviation from a regular activity or course.</em></li>
    </ul>
    <p>Origin: from the Latin verb <em>excurrere</em> which means <em>to run out</em>. This is also the intended meaning behind the title of this course. I want us to deviate from the usual paths and explore the rich connections between reinforcement learning and other disciplines, in particular: optimization, control theory and simulation. And of course, I'm also hoping that this will be a fun activity for everyone.</p>
    <h2 id="time-and-location">Time and Location</h2>
    <p>Twice a week, on Tuesday from 9:30 to 11:30AM and on Friday from 13h30 to 15h40. The course is taught at <a href="https://mila.quebec/">Mila</a> in the Agora of the <a href="https://goo.gl/maps/8xD9WVFZKdNr633CA">6650 Saint-Urbain</a>. You don't need badge access to enter the classroom. Here's a <a href="https://photos.app.goo.gl/StJzkjokQqQ9eqSw5">video</a> showing you how to access the classroom from Saint-Zotique Ouest.</p>
    <h2 id="evaluation">Evaluation</h2>
    <p>The following evaluation structure is subject to change depending on the class size.</p>
    <ul>
    <li>Class participation: 5%</li>
    <li>Assignments: 3 x 15% = 45%</li>
    <li>Final project: 50%</li>
    </ul>
    <h2 id="recommended-textbooks">Recommended Textbooks</h2>
    <p>There is no mandatory textbook. I will however be referencing content from:</p>
    <ul>
    <li><a href="https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887">Markov Decision Processes: Discrete Stochastic Dynamic Programming</a> by Martin Puterman</li>
    <li><a href="https://web.mit.edu/dimitrib/www/RLbook.html">Reinforcement Learning and Optimal Control</a> by Dimitri Bertsekas</li>
    <li><a href="http://incompleteideas.net/book/the-book.html">Reinforcement Learning: An Introduction</a> by Richard S. Sutton and Andrew G. Barto</li>
    <li><a href="https://onlinelibrary.wiley.com/doi/book/10.1002/9780470230381">Simulation and the Monte Carlo Method, Second Edition</a> by Rubinstein and Kroese (2008)</li>
    <li><a href="http://www.athenasc.com/nonlinbook.html">Nonlinear Programming: 3rd Edition</a> by Dimitri Bertsekas</li>
    <li><a href="https://epubs.siam.org/doi/book/10.1137/1.9780898718577">Practical Methods for Optimal Control and Estimation Using Nonlinear Programming</a> by John T. Betts</li>
    <li><a href="https://epubs.siam.org/doi/book/10.1137/1.9780898717761">Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation, Second Edition</a> by Andreas Griewank and Andrea Walther</li>
    </ul>
    <h2 id="schedule">Schedule</h2>
    <h3 id="tuesday-january-7">Tuesday January 7</h3>
    <p>First class. Markov Decision Processes, induced process</p>
    <h3 id="friday-january-10">Friday January 10</h3>
    <p>Examples of MDPs, constrained MLE as sequential allocation, criteria: finite horizon, infinite horizon, average reward, random horizon interpretation of infinite discounted setting. Bellman optimality</p>
    <h3 id="tuesday-january-14">Tuesday January 14</h3>
    <ul>
    <li>Absorbing states and discounting.
    <ul>
    <li>Section 5.3, proposition 5.3.1 and page 127 in Puterman (1994)</li>
    <li><a href="https://ieeexplore.ieee.org/document/917668">Death and discounting</a> by Adam Shwartz</li>
    <li><pre><code>@article{Shwartz2001,
    doi = {10.1109/9.917668},
    url = {https://doi.org/10.1109/9.917668},
    year = {2001},
    month = apr,
    publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
    volume = {46},
    number = {4},
    pages = {644--647},
    author = {A. Shwartz},
    title = {Death and discounting},
    journal = {{IEEE} Transactions on Automatic Control}
    }</code></pre></li>
    </ul></li>
    <li><p>Vector notation: Section 5.6 in Puterman (1994)</p></li>
    <li><p>Neumann series and policy evaluation: Theorem 6.1.1 and corrolary C.4 in Puterman (1994)</p></li>
    <li><p>Matrix splitting methods:</p>
    <ul>
    <li><a href="http://www.sam.math.ethz.ch/~mhg/unt/SWNLA/itmethSWNLA08.pdf">Iterative Methods</a> by Martin H. Gutknecht</li>
    <li><a href="https://atrium.umontreal.ca/primo-explore/search?query=any,contains,Matrix%20iterative%20analysis&amp;tab=default_tab&amp;search_scope=Tout_sauf_articles&amp;sortby=date&amp;vid=UM&amp;facet=frbrgroupid,include,340907508&amp;lang=fr_FR&amp;offset=0">Matrix iterative analysis</a> by Richard Varga</li>
    <li><pre><code>@book{varga1962,
    title       = {Matrix iterative analysis},
    author  = {Richard S. Varga},
    lccn        = {62021277},
    year        = {1962},
    publisher   = {Prentice-Hall},
    address = {Englewood Cliffs}
    }</code></pre></li>
    </ul></li>
    </ul>
    <h3 id="friday-january-17">Friday January 17</h3>
    <ul>
    <li><p>Primal and Dual LP formulations: section 6.9.1 in Puterman (1994)</p>
    <ul>
    <li><a href="https://atrium.umontreal.ca/primo-explore/fulldisplay?docid=UM-ALEPH001676491&amp;context=L&amp;vid=UM&amp;search_scope=Tout_sauf_articles&amp;tab=default_tab&amp;lang=fr_FR">Finite State Markovian Decision Processes</a> by Cyrus Derman</li>
    <li><pre><code>@book{Derman1970,
     author = {Derman, Cyrus},
     title = {Finite State Markovian Decision Processes},
     year = {1970},
     isbn = {0122092503},
     publisher = {Academic Press, Inc.},
     address = {Orlando, FL, USA},
    } </code></pre></li>
    <li><a href="https://www.math.leidenuniv.nl/~kallenberg/Lecture-notes-MDP.pdf">Markov Decision Processes</a> by Lodewijk Kallenberg</li>
    <li><a href="https://www-sop.inria.fr/members/Eitan.Altman/TEMP/h.pdf">Constrained Markov Decision Processes</a> by Eitan Altman</li>
    <li><pre><code>@book{altman1999,
      author    = {Altman, E.},
      publisher = {Chapman and Hall},
      title     = {Constrained Markov Decision Processes},
      year      = 1999,
      pages     = 256
    }</code></pre></li>
    </ul></li>
    <li><p>Occupation measures, induced randomized stationary policy: theorem 6.9.1 in Puterman (1994)</p></li>
    <li><p>Characterization of basic feasible solutions as Markov Deterministic decision rules: proposition 6.9.3 in Puterman (1994)</p></li>
    <li><p>Bias of the MRP in the average reward formulation: section 8.2.1 in Puterman (1994)</p></li>
    <li>Laurent series expansion in the average reward forumulation: section 8.2.2 in Puterman (1994)
    <ul>
    <li>Representation of the total expected discounted reward in terms of the interest rate</li>
    <li>Corollary 8.2.4: decomposition of the total expected discounted reward of a policy into the gain and bias terms</li>
    <li>Corollary 8.2.5: gain as the limit of discount factor going to 1 in the total expected discounted reward</li>
    </ul></li>
    <li>Bellman optimality equations in the total expected discounted reward criterion
    <ul>
    <li>Equality between taking the maximum over the set of deterministic Markovian decision rules instead of the randomized Markovian decision rules: proposition 6.2.1 in Puterman (1994)</li>
    <li>Contraction mappings: section 6.2.3 in Puterman (1994)</li>
    <li>Banach fixed-point theorem: theorem 6.2.3 in Puterman (1994)</li>
    </ul></li>
    </ul>
    <h3 id="tuesday-january-21">Tuesday January 21</h3>
    <ul>
    <li>Contraction mapping theorem: theorem 6.2.3 of Puterman (1994)</li>
    <li>The Bellman optimality operator is a contraction: proposition 6.2.4</li>
    <li>Conserving decision rules: section 6.2.4</li>
    <li>Existence of stationary policies: theorem 6.2.7</li>
    <li>Value iteration: section 6.3.2</li>
    <li>Termination criterion: theorem 6.3.1 c) and d)</li>
    </ul>
    <h3 id="friday-january-24">Friday January 24</h3>
    <ul>
    <li>Policy Iteration: section 6.4
    <ul>
    <li>Monotonicity of the value iterates: proposition 6.4.1</li>
    <li>Convergence of policy iteration: theorem 6.4.2</li>
    <li>PI as a constructive proof for the existence of a solution to the Bellman optimality equations</li>
    </ul></li>
    <li>Newton-Kantorovich Iterations: section 6.4.3
    <ul>
    <li>Gateaux derivatives</li>
    <li>Policy iteration as Newton-Kantorovich: proposition 6.4.4</li>
    <li>See <a href="https://doi.org/10.1016/c2013-0-03044-7">Functional Analysis 2nd edition</a> by Kantorovich and Akilov (1982)</li>
    <li><pre><code>  @book{KantorovichAkilov1982,
      title={Functional Analysis},
      author={Kantorovich, L.V. and Akilov, G.P.},
      isbn={9780080230368},
      lccn={80041734},
      year={1982},
      publisher={Pergamon Press}
    }</code></pre></li>
    <li><p>See <a href="https://www.jstor.org/stable/3689239">On the Convergence of Policy Iteration in Stationary Dynamic Programming</a> by Puterman and Brumelle (1979)</p></li>
    <li><pre><code>@article{PutermanBrumelle1979,
      author = {Martin L. Puterman and Shelby L. Brumelle},
      title = {On the Convergence of Policy Iteration in Stationary Dynamic Programming},
      journal = {Mathematics of Operations Research},
      volume = {4},
      number = {1},
      pages = {60-69},
      year = {1979}
    }</code></pre></li>
    <li><p>See <a href="https://doi.org/10.1016/s1574-0021(96)01016-7">Numerical dynamic programming in economics</a> by John Rust (1996)</p></li>
    <li><pre><code>@incollection{Rust1996,
      doi = {10.1016/s1574-0021(96)01016-7},
      url = {https://doi.org/10.1016/s1574-0021(96)01016-7},
      year = {1996},
      publisher = {Elsevier},
      pages = {619--729},
      author = {John Rust},
      title = {Chapter 14 Numerical dynamic programming in economics},
      booktitle = {Handbook of Computational Economics}
    }</code></pre></li>
    </ul></li>
    <li><p>Modified policy iteration: section 6.5</p>
    <ul>
    <li>Generalized Bellman optimality operator</li>
    <li>See <a href="https://research.tue.nl/en/publications/stopping-times-and-markov-programming-2">Stopping times and Markov programming</a> by Van Nunen 1976</li>
    <li><pre><code>@TechReport{vannunen1976,
    author  = &quot;J.A.E.E. van Nunen and J. Wessels&quot;,
    title       = &quot;Stopping times and Markov programming&quot;,
    institution = &quot;Technische Hogeschool Eindhoven&quot;,
    address = &quot;Eindhoven:&quot;,
    number  = &quot;76-22&quot;,
    year        = &quot;1976&quot;
    }</code></pre></li>
    <li>See <a href="https://research.tue.nl/en/publications/the-generation-of-successive-approximation-methods-for-markov-dec-2">The generation of successive approximation methods for Markov decision processes by using stopping times</a> by Van Nunen and Wessels (1976)</li>
    <li><pre><code>@techreport{vannunentechreport1976,
    author = {van Nunen, J. A. E. E. and Wessels, J.},
    year = {1976},
    title = {The generation of successive approximation methods for Markov decision processes by using stopping times},
    institution = {Eindhoven: Technische Hogeschool Eindhoven.},
    number = {Memorandum COSOR; Vol. 7622}
    }</code></pre></li>
    </ul></li>
    </ul>
    <h3 id="tuesday-january-28">Tuesday January 28</h3>
    <ul>
    <li>Derivation of the smooth Bellman optimality equations from the point of view of a noisy reward function where the noise process comes from a Gumbel distribution</li>
    <li>Social surplus function</li>
    <li>Williams-Daly-Zachary theorem</li>
    <li>See McFadden's <a href="https://eml.berkeley.edu/~mcfadden/discrete/ch5.pdf">Econometric Models of Probabilistic Choice</a> (1981)</li>
    <li>See Rust (1996) and:</li>
    <li><pre><code>  @article{Rust1988,
      doi = {10.1137/0326056},
      url = {https://doi.org/10.1137/0326056},
      year = {1988},
      month = sep,
      publisher = {Society for Industrial {\&amp;} Applied Mathematics ({SIAM})},
      volume = {26},
      number = {5},
      pages = {1006--1024},
      author = {Rust John},
      title = {Maximum Likelihood Estimation of Discrete Control Processes},
      journal = {{SIAM} Journal on Control and Optimization}
    }</code></pre></li>
    <li>Introduction to the projected bellman equations, Galerkin method, variational formulation</li>
    </ul>
    <h3 id="friday-january-31">Friday January 31</h3>
    <ul>
    <li>Lemma 6.3.1 on the property of the weighted Euclidean norm taken under a stationary distribution</li>
    <li>Proposition 6.3.1 using Lemma 6.3.1 to show that the projected policy evaluation operator is a <span class="math inline">\(\gamma\)</span>-contraction and proof of the bound on the error between the fixed-point solution and the true value function</li>
    <li>Use of the Pythagorean theorem (footnote on page 425, section 6.3.1)</li>
    <li>Matrix form of the projected equations (section 6.3.2)</li>
    </ul>
    <h3 id="tuesday-february-4">Tuesday February 4</h3>
    <ul>
    <li>Projection by Monte Carlo simulation, section 6.1.4 of Bertsekas</li>
    <li>Derivation of the closed-form solution of the projection under the weighted Euclidean norm</li>
    <li>Monte Carlo estimates of the closed-form solution (see page 406 of Bertsekas)</li>
    <li>Simulated-based version of the direct solution to the projected policy evaluation equations: LSTD(0)</li>
    <li>Section 6.3.3 of Bertsekas</li>
    <li>Section 6.3.4 of Bertsekas</li>
    </ul>
    <h3 id="friday-february-7">Friday February 7</h3>
    <ul>
    <li>Assignment 1 released</li>
    <li>Perron-Frobenius theorem and numerical methods for computing the stationary distribution</li>
    <li>Projected Value Iteration algorithm, see section 6.3.2 of Bertsekas</li>
    <li>Simulated-based counterpart to PVI: LSPE(0) see section 6.3.4 of Bertsekas</li>
    </ul>
    <h3 id="tuesday-february-11">Tuesday February 11</h3>
    <ul>
    <li>Oblique Projections</li>
    <li>Projected Bellman equations with matrix splitting methods</li>
    <li>Simulated-based counterpart: LSTD(<span class="math inline">\(\lambda\)</span>)</li>
    </ul>
    <h3 id="friday-february-14">Friday February 14</h3>
    <ul>
    <li>Valentine's day</li>
    <li>See (Wikipedia](https://en.wikipedia.org/wiki/Valentine%27s_Day)</li>
    <li>ODE Method and Stochastic Approximation</li>
    <li>Fitted value methods:</li>
    <li>Non-linear case</li>
    <li>Counterexample</li>
    </ul>
    <h3 id="tuesday-february-18">Tuesday February 18</h3>
    <ul>
    <li>Intuition for stochastic approximation. Robbins-Monro algorithm
    <ul>
    <li>Root-finding problems</li>
    <li>Newton's method (live coding)</li>
    <li>Martingale differences</li>
    <li>See sections 1.1.1 and 1.1.2 of Kushner and Yin (2003)</li>
    </ul></li>
    <li>ODE of Q-learning in the tabular setting
    <ul>
    <li>Martingale differences for Q-learning</li>
    <li>See page 44 of Kushner and Yin (2003)</li>
    </ul></li>
    <li>Euler's method
    <ul>
    <li>See Nocedal and Wright (2006)</li>
    </ul></li>
    </ul>
    <h3 id="friday-february-21">Friday February 21</h3>
    <ul>
    <li>Martingale differences in stochastic approximation (continued)
    <ul>
    <li>Section 1.1.2 of Kushner and Yin (2003)</li>
    <li>Demonstration with colab notebook</li>
    </ul></li>
    <li>Derivative approximation with finite differences</li>
    <li>Forward-difference/one-sided-difference</li>
    <li>Central-difference formula</li>
    <li>Taylor's Theorem: see thm. 2.1 in Nocedal and Wright 2006</li>
    <li><p>Section 8.1 of Nocedal and Wright (2006)</p></li>
    <li>Effect of noise on finite differences</li>
    <li>See section 9.1 of Nocedal and Wright (2006)
    <ul>
    <li>Theorem 9.1 of Nocedal and Wright</li>
    </ul></li>
    <li><p>Intro to the likelihood ratio method</p></li>
    </ul>
    <h3 id="tuesday-february-25">Tuesday February 25</h3>
    <ul>
    <li>The likelihood ratio method</li>
    <li><h2 id="see-lecuyer-1990">See L'Ecuyer 1990</h2>
    <pre><code>@article{LEcuyer1990,
      doi = {10.1287/mnsc.36.11.1364},
      url = {https://doi.org/10.1287/mnsc.36.11.1364},
      year = {1990},
      month = nov,
      publisher = {Institute for Operations Research and the Management Sciences ({INFORMS})},
      volume = {36},
      number = {11},
      pages = {1364--1383},
      author = {Pierre L{\textquotesingle}Ecuyer},
      title = {A Unified View of the {IPA},  {SF},  and {LR} Gradient Estimation Techniques},
      journal = {Management Science}
    }</code></pre></li>
    <li>Distributional vs structural parameters
    <ul>
    <li>Section 7.1 of Rubinstein and Kroese (2008)</li>
    </ul></li>
    <li>The score function estimator as a subcase of LR
    <ul>
    <li>See the L'Ecuyer paper and section 7.2 of Rubinstein and Kroese (2008)</li>
    </ul></li>
    <li>Connection between the score function and the Fisher information matrix
    <ul>
    <li>See section 1.14.4 of Rubinstein and Kroese (2008)</li>
    </ul></li>
    <li>The connection between the score function and maximum likelihood estimation
    <ul>
    <li>See section 1.14.3 of Rubinstein and Kroese (2008)</li>
    </ul></li>
    </ul>
    <h3 id="friday-february-28">Friday February 28</h3>
    <ul>
    <li>Recap of the likelihood ration method</li>
    <li>Transformations of random variables</li>
    <li>See section 1.7.2 of Rubinstein and Kroese (2008)</li>
    <li>The inverse transform method
    <ul>
    <li>See section 2.3.1 of Rubinstein and Kroese (2008)</li>
    <li>Example with the exponential distribution (live coding)</li>
    <li>See section 2.4.1.1 of Rubinstein and Kroese (2008)</li>
    </ul></li>
    <li>From LR to IPA:
    <ul>
    <li>See L'Ecuyer (1990)</li>
    </ul></li>
    <li>Derivation and empirical comparision of LR and IPA estimators for the exponential distribution
    <ul>
    <li>See colab notebook (shared in the mailing list)</li>
    </ul></li>
    </ul>
    <h3 id="week-of-march-2">Week of March 2</h3>
    <p>Spring break</p>
    <h3 id="week-of-march-9">Week of March 9</h3>
    <p>Policy gradients: application for learning temporal abstractions, the option-critic architecture, hierarchical and goal-conditioned RL</p>
    <h3 id="week-of-march-16">Week of March 16</h3>
    <p>Policy gradients: Linear-Quadratic Regulator, Lagrangian formulation, MPC, Monte-Carlo Tree Search</p>
    <h3 id="week-of-march-23">Week of March 23</h3>
    <p>Automatic differentiation as discrete-time optimal control</p>
    <h3 id="week-of-march-30">Week of March 30</h3>
    <p>Formulation of inverse RL and meta-RL as bilevel optimization.</p>
    <h3 id="week-of-april-6">Week of April 6</h3>
    <p>Methods (contd.): KKT &quot;trick&quot;, forward, reverse, implicit, competitive. Case studies</p>
    <h3 id="week-of-april-13">Week of April 13</h3>
    <p>Challenge and opportunities</p>
    <h3 id="week-of-april-20">Week of April 20</h3>
    <p>Final project presentations</p>
    </div>
    <h2 id="wellbeing">Wellbeing</h2>
    <p>Academic life can sometimes be overwhelming. Don't hesitate to find support:</p>
    <ul>
    <li>UdeM: <a href="http://www.cscp.umontreal.ca">Centre de santé et de consultation psychologique</a></li>
    <li>McGill: <a href="https://www.mcgill.ca/wellness-hub/">Student wellness hub</a></li>
    <li>Polytechnique: <a href="https://www.polymtl.ca/soutien/">Soutien à la réussite</a></li>
    <li>HEC: <a href="https://www.hec.ca/en/students/support-resources/psychological-support/index.html">Psychological support and ressources</a></li>
    </ul>
    </div>
    </div>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
     (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
       m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
         })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-68825837-1', 'auto');
    ga('send', 'pageview');

    </script>
  </body>
</html>
