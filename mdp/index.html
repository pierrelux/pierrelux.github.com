<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

        <title>Markov decision processes</title>
        <link href="http://fonts.googleapis.com/css?family=Crimson+Text:400,400italic,700,700italic" rel='stylesheet' type='text/css' />
    <link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel='stylesheet' type='text/css'>
    <link href="http://getbootstrap.com/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="/css/custom.css" rel="stylesheet">

        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
      </head>

  <body>
    <nav class="navbar navbar-default navbar-custom">
        <div class="container-fluid">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                </button>
                <a class="page-scroll navbar-brand" href="/">Pierre-Luc Bacon</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a class="page-scroll" href="/">Research</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="/notes">Notes</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
    </nav>
    <div class="container container-article">
    <div class="page-header">
                <h1>Markov decision processes</h1>
                <p class="date">June 1st 2015, Pierre-Luc Bacon</p>
    </div>
    <p>The problem of optimal sequential decision making was studied by Richard Bellman at Los Alamos during the Manhattan Project of World War II. The name <em>Markov Decision Process</em> and most of its terminology is attributed to <span class="citation">(Bellman 1954)</span> and the core concepts were further developed in <span class="citation">(Bellman 1957)</span>.</p>
    <p>A discrete time finite state and action Markov Decision Process (MDP) is a tuple <span class="math inline">\(\langle \mathcal{S}, \mathcal{A}, \mathbb{P}\left\{ \cdot \,\middle|\, s, a\right\}, r\rangle\)</span> with <span class="math inline">\(\mathcal{S}\)</span> denoting the set of states, <span class="math inline">\(\mathcal{A}\)</span> the set of actions, <span class="math inline">\(r:\mathcal{S} \times \mathcal{A} \to \mathbb{R}\)</span> the reward function and <span class="math inline">\(\mathbb{P}\left\{ \cdot \,\middle|\, s, a\right\}\)</span> the probability distribution over the next state given that action <span class="math inline">\(a\)</span> is taken in state <span class="math inline">\(s\)</span>.</p>
    <p>A deterministic policy is a mapping of the form <span class="math inline">\(\pi: \mathcal{S} \to \mathcal{A}\)</span> and stochastic policy is a probability distribution over state-action pairs <span class="math inline">\(\pi: \mathcal{S} \times \mathcal{A} \to [0, 1]\)</span>. We will adopt the following convention in order to provide a unified treatment of both kind of policies. The state transition probabilities under a deterministic policy will be written as</p>
    <p><span class="math display">\[
    \begin{equation}
    \mathbb{P}_\pi\left\{ s^\prime \,\middle|\, s \right\} = \mathbb{P}\left\{ s^\prime \,\middle|\, s, \pi(s) \right\} \notag
    \end{equation}
    \]</span></p>
    <p>In the stochastic case, we will need to marginalize over the set of actions: <span class="math display">\[
    \begin{equation}
    \mathbb{P}_\pi\left\{ s^\prime \,\middle|\, s \right\} = \sum_{a \in \mathcal{A}} \mathbb{P}\left\{ s^\prime \,\middle|, s, a \right\} \pi(s, a) \notag
    \end{equation}
    \]</span></p>
    <p>The reward function will undergoe the same transformation with <span class="math inline">\(r_\pi(s) = r(s, \pi(s))\)</span> in the deterministic case and <span class="math inline">\(r_\pi(s) = \sum_{a \in \mathcal{A}} r(s, a) \pi(s, a)\)</span> for stochastic policies.</p>
    <p>Markov decision problems are usually concerned with two main questions: evaluating the value of a given policy or finding an optimal one. The optimization objective that we will pursue below is <strong>the infinite discounted cumulative reward</strong> defined in expectation as:</p>
    <p><span class="math display">\[
    \begin{equation}
    V^\pi_\gamma(s) = \mathbb{E}_s^\pi\left\{ \sum_{t=1}^\infty \gamma^{t-1} r(S_t, A_t) \right\} \label{eq:inf-discounted-crit}
    \end{equation}
    \]</span></p>
    <p>This particular model was first studied by <span class="citation">(Howard 1960)</span>, but early references can also be found in <span class="citation">(Bellman 1957)</span> under simplified assumptions.</p>
    <p>Fixing the value of <span class="math inline">\(\gamma &lt; 1\)</span> leads to a kind of return where the immediate reward is worth exponentially more than the far future. In this case, the resulting MDP is called a <em>discounted MDP</em>. On the other hand, setting <span class="math inline">\(\gamma = 1\)</span> makes the immediate reward at each step equally important: one then talks of an <strong>undiscounted MDP</strong>.</p>
    <p>Expanding the expectation, we see that</p>
    <p><span class="math display">\[
    \begin{equation}
    V_\gamma^\pi(s) = r_\pi\left(s\right) + \sum_{s^\prime \in \mathcal{S}} \gamma \mathbb{P}_\pi\left\{s^\prime \,\middle|\, s\right\} V_\gamma^\pi(s^\prime) \label{eq:value-pi}
    \end{equation}
    \]</span></p>
    <p>It will also be useful to think of the above in vector form and define <span class="math inline">\(\mathbf{r}_\pi \in \mathbb{R}^{|\mathcal{S}|}\)</span> whose <span class="math inline">\(s\)</span> component is <span class="math inline">\(r_\pi(s)\)</span> and <span class="math inline">\(\mathbf{P}_\pi \in \mathbb{R}^{|\mathcal{S}| \times |\mathcal{S}|}\)</span> with its <span class="math inline">\((s, s^\prime)\)</span> entry being <span class="math inline">\(\mathbb{P}_\pi\left\{s^\prime \,\middle|\, s\right\}\)</span>. The value function <span class="math inline">\(\mathbf{v}_\gamma^\pi \in \mathbb{R}^{|\mathcal{S}|}\)</span> for policy <span class="math inline">\(\pi\)</span> is then written as</p>
    <p><span class="math display">\[
    \begin{equation}
    \mathbf{v}^\pi_\gamma = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}_\gamma^\pi \label{eq:vec-infdiscounted}
    \end{equation}
    \]</span></p>
    <h2 id="existence-of-solution-for-the-evaluation-problem">Existence of solution for the evaluation problem</h2>
    <p>In the evaluation problem, we are interested in finding the value of a given policy under the chosen model: in our case, the infinite discounted formulation. Rearranging the above equation, we get</p>
    <p><span class="math display">\[
    \begin{align}
    \mathbf{v}_\gamma^\pi - \gamma \mathbf{P}_\pi \mathbf{v}_\gamma^\pi &amp;= \mathbf{r}_\pi \notag \\
    (I - \gamma \mathbf{P}_\pi)\mathbf{v}_\gamma^\pi &amp;= \mathbf{r}_\pi \notag \\
    \mathbf{v}_\gamma^\pi &amp;= (I - \gamma\mathbf{P}_\pi)^{-1} \mathbf{r}_\pi \label{eq:mdp-eval-inv}
    \end{align}
    \]</span></p>
    <p>This begs the question of whether the matrix inversion exists. Before giving an answer, we first need to define the <a href="http://en.wikipedia.org/wiki/Spectral_radius">spectral radius</a> of a linear operator <span class="math inline">\(\mathbf{A}\)</span> as</p>
    <p><span class="math display">\[
    \begin{align}
    \rho(\mathbf{A}) = \lim_{n\to \infty} \|\mathbf{A}^n\|^{1/n}
    \end{align}
    \]</span></p>
    <p>Note that the spectral radius does not satisfies the <a href="http://en.wikipedia.org/wiki/Norm_(mathematics)#Definition">definition</a> of a norm. A useful result relates the spectral radius of an operator with the existence of an inverse of the kind appearing above (we will omit its proof for conciseness). It establishes that if <span class="math inline">\(\rho(\mathbf{A}) &lt; 1\)</span> for some linear operator <span class="math inline">\(\mathbf{A}\)</span>, then <span class="math inline">\((I - \mathbf{A})^{-1}\)</span> exists and satisfies</p>
    <p><span class="math display">\[
    \begin{equation}
    (I - \mathbf{A})^{-1} = \lim_{N \to\infty} \sum_{n=0}^N \mathbf{A}^n
    \end{equation}
    \]</span></p>
    <p>For most of the results on MDPs, the following norm is used</p>
    <p><span class="math display">\[
    \begin{equation}
    \|\mathbf{A}\| = \max_{i \in \mathcal{S}} \sum_{j \in \mathcal{S}} |\mathbf{A}_{ij}|
    \end{equation}
    \]</span></p>
    <p>Since <span class="math inline">\(\mathbf{P}_\pi\)</span> is stochastic, all rows must add up to one and <span class="math inline">\(\|\mathbf{P}_\pi\| = 1\)</span>. For <span class="math inline">\(0 \leq \gamma &lt; 1\)</span>, then <span class="math inline">\(1 &gt; \|\gamma \mathbf{P}_\pi \| \geq \rho(\gamma \mathbf{P}_\pi)\)</span>. Hence <span class="math inline">\((I - \gamma \mathbf{P}_\pi)^{-1}\)</span> exists.</p>
    <h2 id="solving-the-optimal-control-problem">Solving the optimal control problem</h2>
    \begin{figure}[ht!]
    \centering
    \input{fig/control}
    \caption{The control mathbb{P}lem aims at finding an optimal policy i.e. one for which the
    corresponding value function is optimal. There might be multiple optimal policies, but
    the optimal value function must be unique.}
    \label{fig:control-mathbb{P}lem}
    \end{figure}
    <p>A policy is said to be optimal when its corresponding value function satisfies</p>
    <p><span class="math display">\[
    \begin{equation}
    V^\star_\gamma(s) = \max_{a \in \mathcal{A}} r(s,a) + \sum_{s^\prime \in \mathcal{S}} \gamma \mathbb{P}\left\{s^\prime \,\middle|\, s, a \right\} V^\star_\gamma(s^\prime) \label{eq:bellman-eq}
    \end{equation}
    \]</span></p>
    <p>Since the above equations must hold for every <span class="math inline">\(s \in \mathcal{S}\)</span>, we are in fact dealing with a system of equations refered to as the <strong>Bellman equations</strong> or the <em>optimality equations</em>. Bellman is credited for discovering the principle of optimality that he described in <span class="citation">(Bellman 1957})</span> as:</p>
    <blockquote>
    <p>An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.</p>
    </blockquote>
    <p>Futhermore, Bellman showed that in the undiscounted finite horizon model, Markov decision problems can be solved by <strong>backward induction</strong> or <strong>dynamic programming</strong> (DP). For a <span class="math inline">\(T\)</span> steps finite horizon, the DP algorithm goes as follow:</p>
    <ol style="list-style-type: decimal">
    <li>Set <span class="math inline">\(t = T\)</span>, and <span class="math inline">\(V_t(s_t) = r(s_t)\)</span></li>
    <li>For <span class="math inline">\(t = T-1\)</span> down to 1 compute <span class="math display">\[
    \begin{equation}
    V_t(s_t) = \max_{a \in \mathcal{A}} r(s_t, a) + \sum_{s^\prime \in \mathcal{S}} \mathbb{P}\left\{ s^\prime \,\middle|\, s_t, a \right\} V_{t+1}(s^\prime)
    \end{equation}
    \]</span></li>
    </ol>
    <p>The power of DP lies in the fact that the value function depends on the history of the process only through the current state rather than the exponentially larger set of histories. This is possible under the setting where the action set, rewards and transitions only depend on the current state.</p>
    <p>In the infinite discounted case, the direct application of DP would be unpractical. Fortunately, we will be able to find an analogue of the backward induction approach but for which the sequence of intermediary policies for each <span class="math inline">\(t\)</span> will not have to be saved. We will in fact obtain a <strong>stationary policy</strong> applicable for every <span class="math inline">\(t\)</span>. As a first step to towards this goal, we will need to think of the <span class="math inline">\(\max\)</span> as a non-linear operator acting on a value function <span class="math inline">\(\mathbf{v} \in \mathbb{R}^{|\mathcal{S}|}\)</span> as follow</p>
    <p><span class="math display">\[
    \begin{equation}
    T\mathbf{v} = \max_{a \in \mathcal{A}_s} r(s,a) + \sum_{s^\prime \in \mathcal{S}} \gamma \mathbb{P}\left\{s^\prime \,\middle|\, s, a \right\} \mathbf{v}(s^\prime)
    \end{equation}
    \]</span></p>
    <p>The name <strong>Bellman operator</strong> is generally associated with <span class="math inline">\(T\)</span>. A solution to the Bellman equations is called a <strong>fixed point</strong> and satisfies the <strong>fixed point equation</strong>:</p>
    <p><span class="math display">\[
    \begin{equation}
    T\mathbf{v} = \mathbf{v}
    \end{equation}
    \]</span></p>
    <p>The existence of fixed points is provided in Banach spaces through the Banach fixed point theorem. A <a href="http://en.wikipedia.org/wiki/Banach_space">Banach space</a> is a complete linear vector space equiped with a norm. The meaning of <strong>complete</strong> is in the sense of the existence of a limit to <a href="http://en.wikipedia.org/wiki/Cauchy_sequence">Cauchy sequences</a> in that space. A Cauchy sequence is a subset of <span class="math inline">\(\{v \} \in V\)</span> of a vector space <span class="math inline">\(V\)</span> for which its elements become arbitrarily close with respect to the given norm.</p>
    <p>We therefore want to think as the space of bounded real-valued value functions as a Banach space. Another necessary concept for the understanding of the Banach fixed point theorem is that of a contraction mapping. <span class="math inline">\(T : V \to V\)</span> for a Banach space <span class="math inline">\(V\)</span> is a contraction mapping if there exists a <span class="math inline">\(\lambda, 0 \leq \lambda &lt; 1\)</span> such that for any <span class="math inline">\(u,v \in V\)</span></p>
    <p><span class="math display">\[
    \begin{equation}
    \|Tv - Tu\| \leq \lambda \|v - u\|
    \end{equation}
    \]</span></p>
    <p>The <a href="http://en.wikipedia.org/wiki/Banach_fixed-point_theorem">Banach fixed point theorem</a> states that for a contraction mapping <span class="math inline">\(T\)</span> in a Banach space <span class="math inline">\(V\)</span>, there exists a unique <span class="math inline">\(v^\star\)</span> to <span class="math inline">\(Tv^\star = v^\star\)</span>. Furthermore, given an arbitrary initial <span class="math inline">\(v_0\)</span> the iterative application of <span class="math inline">\(T\)</span> converges to <span class="math inline">\(v^\star\)</span>.</p>
    <p>It can be shown that the Bellman operator is indeed a contraction mapping over the space of value functions. Therefore, the Banach fixed point theorem guarantees a unique solution to  and can elegantly be transformed into an algorithm for finding fixed point. Given an arbitary value function, it suffices to apply the Bellman operator and repeat the process until convergence. This algorithm is known as the <a href="http://webdocs.cs.ualberta.ca/~sutton/book/4/node5.html">value iteration</a> algorithm.</p>
    <p>An alternative algorithm called <a href="http://webdocs.cs.ualberta.ca/~sutton/book/4/node4.html">policy iteration</a> (PI) can also solve the control problem. In a PI scheme, two steps are interleaved: policy evaluation and policy improvement. The general idea goes as follow: from an initial policy <span class="math inline">\(\pi_0\)</span> compute its corresponding value function (policy evaluation) and derive the greedy policy <span class="math inline">\(\pi_{k+1}\)</span> from it (policy improvement), then repeat these two steps as necessary.</p>
    <h1 id="references" class="unnumbered">References</h1>
    <div id="refs" class="references">
    <div id="ref-Bellman1954">
    <p>Bellman, Richard. 1954. “The Theory of Dynamic Programming.” <em>Bull. Amer. Math. Soc.</em> 60 (6). American Mathematical Society (AMS): 503–16.</p>
    </div>
    <div id="ref-Bellman1957">
    <p>———. 1957. <em>Dynamic Programming</em>. Princeton, N.J.: Princeton University Press.</p>
    </div>
    <div id="ref-Howard1960">
    <p>Howard, Ronald A. 1960. <em>Dynamic Programming and Markov Processes</em>. Cambridge, Massachusetts: MIT Press.</p>
    </div>
    </div>
    </div>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
     (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
       m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
         })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-68825837-1', 'auto');
    ga('send', 'pageview');

    </script>
  </body>
</html>
